{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a35ba9b",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE TOOL KIT(NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d61001e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a429dc37",
   "metadata": {},
   "source": [
    "# word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14027643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Akash\n",
      "[nltk_data]     Manral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text=\"Akash plays good football\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "946b31be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Akash', 'plays', 'good', 'football']\n"
     ]
    }
   ],
   "source": [
    "#tokenzing\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46a0ac",
   "metadata": {},
   "source": [
    "# sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b84102fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent=\"He is the best player. He should be the captain. He is only 11YR OLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3b52d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He is the best player.', 'He should be the captain.', 'He is only 11YR OLD']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a045c0",
   "metadata": {},
   "source": [
    "# regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bfbf40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "text=\"NLP is fun it can Deal with TEXTS and SOunds but can't deal with IMAGES. We have session at 11AM. We can Learn there a $LOT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eee30804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'fun', 'it', 'can', 'eal', 'with', 'and', 'unds', 'but', 'can', 't', 'deal', 'with', 'e', 'have', 'session', 'at', 'e', 'can', 'earn', 'there', 'a']\n"
     ]
    }
   ],
   "source": [
    "#print word by word that contains all small case starting with a-z\n",
    "print(regexp_tokenize(text,'[a-z]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed447926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 's', 'f', 'u', 'n', 'i', 't', 'c', 'a', 'n', 'e', 'a', 'l', 'w', 'i', 't', 'h', 'a', 'n', 'd', 'u', 'n', 'd', 's', 'b', 'u', 't', 'c', 'a', 'n', \"'\", 't', 'd', 'e', 'a', 'l', 'w', 'i', 't', 'h', 'e', 'h', 'a', 'v', 'e', 's', 'e', 's', 's', 'i', 'o', 'n', 'a', 't', 'e', 'c', 'a', 'n', 'e', 'a', 'r', 'n', 't', 'h', 'e', 'r', 'e', 'a']\n"
     ]
    }
   ],
   "source": [
    "#to separaate every word into letter\n",
    "print(regexp_tokenize(text,\"[a-z']\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ed3f3b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'fun', 'it', 'can', 'eal', 'with', 'and', 'unds', 'but', \"can't\", 'deal', 'with', 'e', 'have', 'session', 'at', 'e', 'can', 'earn', 'there', 'a']\n"
     ]
    }
   ],
   "source": [
    "#to get word like can't ,don't in small case starting wit a-z\n",
    "print(regexp_tokenize(text,\"[a-z']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82317492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'D', 'TEXTS', 'SO', 'IMAGES', 'W', 'AM', 'W', 'L', 'LOT']\n"
     ]
    }
   ],
   "source": [
    "#print all words in capitals staring with A-Z\n",
    "print(regexp_tokenize(text,'[A-Z]+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a9a25d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'D', 'TEXTS', 'SO', \"'\", 'IMAGES', 'W', 'AM', 'W', 'L', 'LOT']\n"
     ]
    }
   ],
   "source": [
    "#to get word like can't ,don't in Capital case starting wit A-Z\n",
    "print(regexp_tokenize(text,\"[A-Z']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6deb9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"NLP is fun it can Deal with TEXTS and SOunds but can't deal with IMAGES. We have session at 11AM. We can Learn there a $LOT.\"]\n"
     ]
    }
   ],
   "source": [
    "#everything in one line\n",
    "print(regexp_tokenize(text,\"[\\a-z']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "73392702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP ', ' ', ' ', ' ', ' D', ' ', ' TEXTS ', ' SO', ' ', ' ', ' ', ' ', ' IMAGES. W', ' ', ' ', ' ', ' 11AM. W', ' ', ' L', ' ', ' ', ' $LOT.']\n"
     ]
    }
   ],
   "source": [
    "#anything starts with caret is not equal in small case(without small case)(only capital case)\n",
    "print(regexp_tokenize(text,\"[^a-z']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40cc3a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' is fun it can ', 'eal with ', ' and ', 'unds but can', 't deal with ', '. ', 'e have session at 11', '. ', 'e can ', 'earn there a $', '.']\n"
     ]
    }
   ],
   "source": [
    "#anything starts with caret is not equal in capital case(without capital case)(only small case)\n",
    "print(regexp_tokenize(text,\"[^A-Z']+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c50080b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"NLP is fun it can Deal with TEXTS and SOunds but can't deal with IMAGES. We have session at \", 'AM. We can Learn there a $LOT.']\n"
     ]
    }
   ],
   "source": [
    "#without numbers\n",
    "print(regexp_tokenize(text,\"[^0-9]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "04264806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11']\n"
     ]
    }
   ],
   "source": [
    "#with numbers(only numbers)\n",
    "print(regexp_tokenize(text,\"[0-9]+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9202ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$']\n"
     ]
    }
   ],
   "source": [
    "#print sign\n",
    "print(regexp_tokenize(text,\"[$]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad3e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7672c511",
   "metadata": {},
   "source": [
    "# STOP WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70eb4d",
   "metadata": {},
   "source": [
    "Stop words are the words which are very common in occurence such as a,an ,the ,etc. We ignore these words during preprocessing part because these word do not give any specific information and instead add to additional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d0dd4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Akash\n",
      "[nltk_data]     Manral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "020d6c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defaukt stopwords in English\n",
    "stopwords= stopwords.words('English')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "37288539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fdf53a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding new stopwords\n",
    "stopwords.extend([\"old\",'new'])\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6031b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0775f33",
   "metadata": {},
   "source": [
    "# similar to stopwords we can also ignore punctuations from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "93ce2242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "punct=string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8db64486",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"According to the writings known as the Puranas (religious/historical texts written down in the 5th century CE), Bharata conquered the whole subcontinent of India and ruled the land in peace and harmony. The land was, therefore, known as Bharatavarsha (`the subcontinent of Bharata'). Hominid activity in the Indian subcontinent stretches back over 250,000 years, and it is, therefore, one of the oldest inhabited regions on the planet.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6673dc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of original text == > 435\n",
      "length of cleaned text == > 40\n",
      "\n",
      " ['According', 'writings', 'known', 'Puranas', 'religious/historical', 'texts', 'written', '5th', 'century', 'CE', 'Bharata', 'conquered', 'whole', 'subcontinent', 'India', 'ruled', 'land', 'peace', 'harmony', 'The', 'land', 'therefore', 'known', 'Bharatavarsha', 'subcontinent', 'Bharata', 'Hominid', 'activity', 'Indian', 'subcontinent', 'stretches', 'back', '250,000', 'years', 'therefore', 'one', 'oldest', 'inhabited', 'regions', 'planet']\n"
     ]
    }
   ],
   "source": [
    "cleaned_text=[]\n",
    "\n",
    "for word in nltk.word_tokenize(text):\n",
    "    if word not in punct:\n",
    "        if word not in stopwords:\n",
    "            cleaned_text.append(word)\n",
    "            \n",
    "print(\"length of original text == >\",len(text))\n",
    "print(\"length of cleaned text == >\",len(cleaned_text))\n",
    "print(\"\\n\",cleaned_text)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de8265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a14fc63",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d574772",
   "metadata": {},
   "source": [
    "Stemming means changing the words or group of words ,either by removing the pre-fixes or suf-fixes, without giving any gramatical value to the meaning of the stem formed after the process.\n",
    "\n",
    "computation>> comput\n",
    "\n",
    "computer>> comp\n",
    "\n",
    "hobbies>> hobbi\n",
    "\n",
    "It brings the word bak to its base word,but the word formed may or may not have any meaning.\n",
    "\n",
    "Two popular stemmers:-\n",
    "\n",
    "1-Porter Stemmer\n",
    "\n",
    "2-Lancaster Stemmer\n",
    "\n",
    "3-Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fe5ffd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Stemmmers\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer\n",
    "porter=PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "snowball=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "455dd0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer\n",
      "hobbi\n",
      "playground\n",
      "exercis\n",
      "*******************************************\n",
      "Lancaster Stemmer\n",
      "hobby\n",
      "playground\n",
      "exerc\n"
     ]
    }
   ],
   "source": [
    "#how it works\n",
    "print(\"PorterStemmer\")\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"playground\"))\n",
    "print(porter.stem(\"exercise\"))\n",
    "print(\"*******************************************\")\n",
    "print(\"Lancaster Stemmer\")\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"playground\"))\n",
    "print(lancaster.stem(\"exercise\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0163dfd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'office',\n",
       " 'on',\n",
       " 'my',\n",
       " 'bike',\n",
       " 'where',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'car',\n",
       " 'hitting',\n",
       " 'on',\n",
       " 'a',\n",
       " 'tree']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"I was going to office on my bike where i saw car hitting on a tree\"\n",
    "\n",
    "token=list(nltk.word_tokenize(sentence))\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5550fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wa go to offic on my bike where i saw car hit on a tree\n",
      "i was going to off on my bik wher i saw car hit on a tre\n",
      "i was go to offic on my bike where i saw car hit on a tree\n"
     ]
    }
   ],
   "source": [
    "for stemmer in(porter,lancaster,snowball):\n",
    "    stem=[stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "50cd2a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wa go to offic on my bike where i saw car hit on a tree\n",
      "i was going to off on my bik wher i saw car hit on a tre\n",
      "i was go to offic on my bike where i saw car hit on a tree\n"
     ]
    }
   ],
   "source": [
    "stemmer=[porter,lancaster,snowball]\n",
    "for s in stemmer:\n",
    "    stem=[s.stem(t) for t in token]\n",
    "    print(\" \".join(stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76848e7d",
   "metadata": {},
   "source": [
    "lancaster algorithm is faster than porter but it is more complex. .Porter is the oldest algorithm\n",
    "\n",
    "Snowball is the updated vesion of Porter and is most popular.It is also available in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cd8955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abfc771e",
   "metadata": {},
   "source": [
    "# LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348945d",
   "metadata": {},
   "source": [
    "Lemmatization does the same thing as stemming and try to bring the word to its base form but it keep in account the actual meaning of base word.\n",
    "\n",
    "The base words is known as Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a7924634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Akash\n",
      "[nltk_data]     Manral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a206de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "playground\n",
      "exercise\n",
      "run\n",
      "runner\n"
     ]
    }
   ],
   "source": [
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "print(lemma.lemmatize(\"running\"))\n",
    "print(lemma.lemmatize(\"playground\"))\n",
    "print(lemma.lemmatize(\"exercise\"))\n",
    "print(lemma.lemmatize(\"runs\"))\n",
    "print(lemma.lemmatize(\"runners\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f25fb",
   "metadata": {},
   "source": [
    "Here,we can se lemma has changed the words for same base\n",
    "\n",
    "This is because we haven't given any context to lemmatizer.\n",
    "\n",
    "Generally, it is given by passing the POS(Parts of Speech) tags for the word in a sentence e.g-v=verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e8288138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "exercise\n",
      "runner\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize(\"running\",pos='v'))\n",
    "print(lemma.lemmatize(\"exercise\",pos='v'))\n",
    "print(lemma.lemmatize(\"runner\",pos='v'))\n",
    "print(lemma.lemmatize(\"runs\",pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960abe3b",
   "metadata": {},
   "source": [
    "Lematizer is complex and takes a lot of time to calculate\n",
    "\n",
    "so it should be used when the real meaning of the word or context is necessary for preprocessing,else stemming should be used\n",
    "\n",
    "it completely depends on the type of problem you are tryng to solve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d41e7532",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-e4c24a9b0aa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stemming for {} is {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mto_lowercase\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mto_lowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m \u001b[0malways\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \"\"\"\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mto_lowercase\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "text=\"King Going NOTHIG running scheduled of runners eating is hobby\"\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "\n",
    "token=[nltk.word_tokenize(text)]\n",
    "\n",
    "for w in token:\n",
    "    print(\"stemming for {} is {}\".format(w, porter.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc293d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039eca06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49007f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bde8ee8",
   "metadata": {},
   "source": [
    "# Word Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66985f8e",
   "metadata": {},
   "source": [
    "word net is english database use to find out antonym synonym of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a93702d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f57d86d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ==> {'combat-ready', 'alive', 'active_voice', 'active', 'active_agent', 'dynamic', 'participating', 'fighting'}\n",
      "antonyms ==> {'passive', 'stative', 'passive_voice', 'inactive', 'dormant', 'quiet', 'extinct'}\n"
     ]
    }
   ],
   "source": [
    "synonyms=[]\n",
    "antonyms=[]\n",
    "\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "            \n",
    "print(\"synonyms ==>\",set(synonyms))\n",
    "print(\"antonyms ==>\",set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cabeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
